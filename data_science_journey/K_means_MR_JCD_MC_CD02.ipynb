{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deniseiras/EscolaVeraoSantosDumont2023/blob/main/K_means_MR_JCD_MC_CD02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introdução ao K-means\n",
        "[link text](https://pt.wikipedia.org/wiki/K-means)\n",
        "![picture](https://drive.google.com/uc?export=view&id=160HEkR11QhXN8id6pJLIvN-6lKQiUwZo)"
      ],
      "metadata": {
        "id": "oLVcI2B6iSay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means Versão spark\n",
        "  Neste exemplo consideramos que geramos 1000 pontos randomicos, no intervalo\n",
        "  [1,5000] e queremos clusterizar em k grupos."
      ],
      "metadata": {
        "id": "JglQBkEgxWFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iniciamos instalando o pyspark"
      ],
      "metadata": {
        "id": "ItQBlXjrxwqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiGEOlx23cED",
        "outputId": "888756bc-d9fb-41e8-aeb2-0f1544342905"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=64155667cb6e479490456c500a22ca5777b426c28afdb1385aba338fa638ea22\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importamos pyspark - API python para Spark\n",
        "##            radom - para geração pesudo-aleatório de inteiros"
      ],
      "metadata": {
        "id": "Thz_F268x52m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark as ps\n",
        "import random"
      ],
      "metadata": {
        "id": "Rk7rU1Ar3xa_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importamos o SparkContext para definição do espaço RDD\n",
        "- Deve existir apenas um SparkContext por job spark\n",
        "- Note que estamos usando a API RDD.. Mais adiante, usaremos Dataframe"
      ],
      "metadata": {
        "id": "N4DI7mN0yN0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.context import SparkContext\n",
        "sc= SparkContext(appName=\"K-means\", master=\"local[4]\")"
      ],
      "metadata": {
        "id": "YHMm1Dw7ACHr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iniciamos um acumulador - Fica armazenado no Driver e pode ser acumulado \n",
        "## a partir dos nós trabalhadores"
      ],
      "metadata": {
        "id": "fA6nXqqjyebn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creates and initializes an accumulator to zero \n",
        "acum=sc.accumulator(0) "
      ],
      "metadata": {
        "id": "M1guqjQ6wqst"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criamos uma classe para iniciar as duas listas:\n",
        "\n",
        "*   centroids - conjunto de centroids dos clustes. Inicialmente vazia\n",
        "*   points - lista de pontos a clusterizar\n",
        "\n",
        "\n",
        "  - "
      ],
      "metadata": {
        "id": "C_X8mtlzyzPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class K_means_exemplo_MR:\n",
        "  def __init__(self, k):\n",
        "    self.centroids=random.sample(range(1,1000),k)\n",
        "    # duas partições - processadores locais no caso\n",
        "    self.points=sc.parallelize(random.sample(range(1,5000),1000),2)\n",
        "  \n"
      ],
      "metadata": {
        "id": "aJ8tmRlCchSp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função findCluster\n",
        "  input= p-> ponto sendo avaliado; centroids-> lista de centroids\n",
        "  Objetivo:\n",
        "    c=varrer a lista e encontrar o centroid mais proximo ao ponto \"p\""
      ],
      "metadata": {
        "id": "HtJf8ZHhzMMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def findCluster(p,centroids):\n",
        "    min=999999\n",
        "    for c in centroids:\n",
        "      newdist=abs(p-c)\n",
        "      if  newdist < min:\n",
        "        centroid=c\n",
        "        min = newdist\n",
        "    return [centroid,p]"
      ],
      "metadata": {
        "id": "qO5lzEWTUV-V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Função K_Means - função principal\n",
        "  obtém a lista de centroids  randômicos - iniciais (centroids_updt)\n",
        "  inicializa a lista de centroids\n",
        "  realiza um loop até que o processo convirja e centroids == centroids_updt\n",
        "     a função map chama a findCluster-> forma o par (centroid, ponto)\n",
        "     utiliza a função groupByKey para gerar (K,[V])\n",
        "     calcula a media com a soma/|V| dos valores em V\n",
        "     transforma a tupla em  RDD com centroids\n",
        "     incrementa no acumulador o numero de iterações"
      ],
      "metadata": {
        "id": "tQJWHGbf04sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_means(k):\n",
        "\n",
        "  k_means = K_means_exemplo_MR(k)\n",
        "  centroids = list([])\n",
        "  centroids_updt = k_means.centroids\n",
        "  print(\"centroids\"+str((centroids)))\n",
        "  print(\"centroid_upd\"+str(centroids_updt))\n",
        "  points=k_means.points\n",
        "  print(str(points.count()))\n",
        "  iter=0\n",
        "  while (sorted(centroids) != sorted(centroids_updt)):\n",
        "    centroids=centroids_updt\n",
        "    keysValues=points.map(lambda j:findCluster(j,centroids))\n",
        "    #MapValues- Passa cada valor do par (key, Values) a função, sem mudar a chave\n",
        "    reducedKeys=keysValues.groupByKey().mapValues(lambda x: sum(x) / len(x))\n",
        "    newKeys=reducedKeys.map(lambda x:x[1])\n",
        "    centroids_updt=newKeys.collect()\n",
        "    acum.add(1)\n",
        "\n",
        "  return centroids_updt"
      ],
      "metadata": {
        "id": "M-w8-SyBSu6l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoca o process"
      ],
      "metadata": {
        "id": "M_uz2S9H21Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k=3\n",
        "groups = k_means(k)\n",
        "print(str(groups)+ \"Number of iterations: \"+str(acum))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGs3ek9liyqK",
        "outputId": "467bdb72-6dfe-4a45-ce67-93093a0f5b6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centroids[]\n",
            "centroid_upd[622, 482, 354]\n",
            "1000\n",
            "[780.7060606060606, 2436.306060606061, 4079.573529411765]Number of iterations: 17\n"
          ]
        }
      ]
    }
  ]
}