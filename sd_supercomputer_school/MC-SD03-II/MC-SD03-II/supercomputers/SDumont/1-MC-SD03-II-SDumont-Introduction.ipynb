{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c007fdcd",
   "metadata": {},
   "source": [
    "# Introduction the CUDAWARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa0d53",
   "metadata": {},
   "source": [
    "Welcome to the webinar _Introduction the CUDAWARE_ in Santos Dummont Summer School 2023 Program. In this webinar you will learn several techniques for scaling single GPU applications to multi-GPU and multiple nodes, with an emphasis on [NCCL (NVIDIA Collective Communications Library)](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html), [CUDAWARE-MPI](https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/), and [NVSHMEM](https://developer.nvidia.com/nvshmem) which allows for elegant multi-GPU application code and has been proven to scale very well on systems with many GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9087395a",
   "metadata": {},
   "source": [
    "## The Coding Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755c507",
   "metadata": {},
   "source": [
    "The first step is display information about the CPU architecture with the command `lscpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cddea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f3558",
   "metadata": {},
   "source": [
    "In this node, we can observe that the multi-GPU resources connect with the NUMA nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686add1",
   "metadata": {},
   "source": [
    "For your work today, you have access to several GPUs in the cloud. Run the following cell to see the GPUs available to you today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901503da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi topo -m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04314c13",
   "metadata": {},
   "source": [
    "While your work today will be on a single node, all the techniques you learn today, in particular CUDAWARE-MPI and NVSHMEM, can be used to run your applications across clusters of multi-GPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e24d1",
   "metadata": {},
   "source": [
    "Let us show the NVLink Status for different GPUs reported from `nvidia-smi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi nvlink --status -i 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf551095",
   "metadata": {},
   "source": [
    "In the end, it gives information about the NUMA memory nodes, with tue `lstopo` command, that is used to show the topology of the system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d71eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lstopo --of png > sdumont.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c96a34-0e22-4767-b2f5-728ce100c1da",
   "metadata": {},
   "source": [
    "This will import and display a .png image in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac6f5a-849b-44ff-8422-4b36c653303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "path=\"sdumont.png\"\n",
    "display(Image.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea3fee",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2677eed",
   "metadata": {},
   "source": [
    "During this short course today you will work through each of the following notebooks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baba471",
   "metadata": {},
   "source": [
    "- [_NCCL_](2-MC-SD03-II-SDumont-NCCL-P2P.ipynb): In this notebook you will introduced the NCCL API, and the concepts of peer-to-peer communication between GPUs.\n",
    "\n",
    "- [_CUDAWARE-MPI_](3-MC-SD03-II-SDumont-CUDAWARE-MPI.ipynb): You will begin by familiarizing with the concepts of CUDAWARE-MPI API to multi-GPU nodes.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - Single GPU_](4-MC-SD03-II-SDumont-MCπ-SGPU.ipynb): You will begin by familiarizing yourself with a single GPU implementation of the monte-carlo approximation of π algorithm, which we will use to introduce many multi GPU programming paradigms.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - Multiple GPUs_](5-MC-SD03-II-SDumont-MCπ-MGPU.ipynb): In this notebook you will extend the monte-carlo π program to run on multiple GPUs by looping over available GPU devices.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - CUDAWARE-MPI_](6-MC-SD03-II-SDumont-MCπ-CUDAWARE-MPI): In this notebook you will learn how to applied the CUDAWARE-MPI, and some concepts about peer-to-peer communication between GPUs in the SPMD paradigm.\n",
    "\n",
    "- [_Monte Carlo Approximation of $\\pi$ - NVSHMEM_](7-MC-SD03-II-SDumont-MCπ-NVSHMEM.ipynb): In this notebook you will be introduced to NVSHMEM, and will take your first pass with it using the monte-carlo π program.\n",
    "\n",
    "- [_Jacobi Iteration_](8-MC-SD03-II-SDumont-Jacobi.ipynb): In this notebook you will be introduced to a Laplace equation solver using Jacobi iteration and will learn how to use NVSHMEM to handle boundary communications between multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0253e-b620-4a20-94aa-560084533755",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b214a1-2003-4737-a971-d3cd66aa7b6a",
   "metadata": {},
   "source": [
    "* NCCL (NVIDIA Collective Communications Library), https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/index.html (accessed January 12, 2023).\n",
    "\n",
    "* CUDAWARE, https://developer.nvidia.com/blog/introduction-cuda-aware-mpi/\n",
    "(accessed January 16, 2023).\n",
    "\n",
    "* NVSHMEM, https://developer.nvidia.com/nvshmem\n",
    "(accessed January 16, 2023).\n",
    "\n",
    "* CZARNUL, P. Parallel Programming for Modern High-Performance Computing Systems. New York: Chapman and Hall: CRC, 2018. 330 p.\n",
    "\n",
    "* DOWD, K.; SEVERANCE, C. An Introduction to Parallel Programming.\n",
    "Sebastopol: O’Reilly & Associates, 1998. 446 p.\n",
    "\n",
    "* HAGER, G.; WELLEIN, G. Introduction to High-Performance Computing for Scientists and Engineers. 2nd ed. Boca Raton: Chapman and Hall: CRC, 2018. 400 p.\n",
    "\n",
    "* HENNESSY, J. L.; PATTERSON, D. A. Computer Architecture: A Quantitative Approach. 6th ed. San Francisco: Elsevier: Morgan Kaufmann, 2017. 936 p.\n",
    "\n",
    "* PACHECO, P. S. An Introduction to Parallel Programming. San Francisco: Elsevier: Morgan Kaufmann, 2011. 392 p. \n",
    "\n",
    "* STERLING, T.; ANDERSON, M.; BRODOWICZ, M. High-Performance Computing: Modern Systems and Practices. Cambridge: Elsevier: Morgan Kaufmann, 2017. 718 p.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
